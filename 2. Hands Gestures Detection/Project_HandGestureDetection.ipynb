{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8f0d31cc",
   "metadata": {},
   "source": [
    "Objectif :\n",
    "    Le but de ce code est de détecter et afficher des gestes de la main en temps réel\n",
    "      à partir d'un flux vidéo de webcam dans un notebook.\n",
    "\n",
    "Voici une description plus détaillée :\n",
    "  - Accès à la webcam et affichage du flux : Le code utilise la webcam de l'utilisateur\n",
    "      pour capturer un flux vidéo en temps réel. Ce flux est ensuite affiché dans\n",
    "      le notebook Colab grâce à une intégration HTML et JavaScript.\n",
    "  - Détection des mains et des points de repère : Grâce à la bibliothèque MediaPipe Hands,\n",
    "      le code identifie les mains présentes dans chaque image du flux vidéo et repère des points\n",
    "      clés sur la main (articulations des doigts, poignet, etc.).\n",
    "  - Reconnaissance de gestes : Le code inclut une fonction detect_gesture qui analyse la position\n",
    "      des points de repère de la main pour reconnaître deux gestes spécifiques : le signe \"OK\"\n",
    "      (pouce et index formant un cercle) et le signe \"Salut\" (main ouverte avec les doigts écartés).\n",
    "  - Affichage des résultats : Les points de repère de la main sont dessinés sur l'image du flux vidéo\n",
    "      pour une visualisation. Si un geste est reconnu, le nom du geste (\"OK\" ou \"Salut\") est affiché sur l'image.\n",
    "  - Interaction en temps réel : Tout ce processus se déroule en temps réel, ce qui permet à l'utilisateur d'interagir\n",
    "      avec le système en effectuant les gestes reconnus devant la webcam.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05fd0bda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not open requirements file: [Errno 2] No such file or directory: 'requirements.txt'\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n",
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "# 0. Preparation Environnement\n",
    "# 0.1. Installer les packages requis\n",
    "!pip install -r requirements.txt \n",
    "!pip install pandas matplotlib seaborn scikit-learn opencv-python mediapipe -q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bba9c3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.2. Charger les librairies \n",
    "import cv2\n",
    "import mediapipe as mp\n",
    "import numpy as np\n",
    "\n",
    "from IPython.display import display, HTML\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5d635535",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Initialisation de MediaPipe Hands\n",
    "\"\"\"\n",
    "Note: \n",
    "    - 'mp_hands': pour récupérer le module de détection des mains dans MediaPipe\n",
    "    - 'hands': pour initialiser le détecteur de mains avec un suil de confiance pour la détection et le suivi\n",
    "    - 'mp_drawing': pour dessiner les points de repère (landmarks) sur l'image\n",
    "\"\"\"\n",
    "\n",
    "mp_hands = mp.solutions.hands\n",
    "hands = mp_hands.Hands(static_image_mode=False, max_num_hands=2, min_detection_confidence=0.7, min_tracking_confidence=0.7)\n",
    "mp_drawing = mp.solutions.drawing_utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "84b15a41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Fonction Détection de Gestes\n",
    "\"\"\"\n",
    "Note: \n",
    "    - 'detect_gesture': fonction qui prend en entrée les landmarks d'une main détectée et retourne le geste reconnu\n",
    "        - OK: Si le bout du pouce est proche de l'index\n",
    "        - Salut: Si tous les doigts (sauf le pouce) sont étendus et au dessus du poignet\n",
    "    - 'thumb_tip', 'index_tip', 'pinky_tip': points de repère pour le bout du pouce, l'index et l'auriculaire\n",
    "    - Conditions pour reconnaître les gestes \"OK\" et \"Salut\"\n",
    "\"\"\"\n",
    "\n",
    "def detect_gesture(hand_landmarks):\n",
    "    thumb_tip = hand_landmarks.landmark[mp_hands.HandLandmark.THUMB_TIP]\n",
    "    index_tip = hand_landmarks.landmark[mp_hands.HandLandmark.INDEX_FINGER_TIP]\n",
    "    pinky_tip = hand_landmarks.landmark[mp_hands.HandLandmark.PINKY_TIP]\n",
    "\n",
    "    # Signe OK : bout du pouce proche de l'index\n",
    "    if abs(thumb_tip.x - index_tip.x) < 0.05 and abs(thumb_tip.y - index_tip.y) < 0.05:\n",
    "        return \"OK\"\n",
    "\n",
    "    # Signe de salut : main ouverte, doigts écartés\n",
    "    fingers_extended = [l.y < hand_landmarks.landmark[mp_hands.HandLandmark.WRIST].y\n",
    "                        for l in [hand_landmarks.landmark[i]\n",
    "                        for i in [mp_hands.HandLandmark.INDEX_FINGER_TIP,\n",
    "                                  mp_hands.HandLandmark.MIDDLE_FINGER_TIP,\n",
    "                                  mp_hands.HandLandmark.RING_FINGER_TIP,\n",
    "                                  mp_hands.HandLandmark.PINKY_TIP]]]\n",
    "    if all(fingers_extended):\n",
    "        return \"Salut\"\n",
    "\n",
    "    return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "098b2941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Démarrer le flux vidéo avec miroir et détection de gestes\n",
    "\"\"\"\n",
    "Note: \n",
    "    - 'display(HTML(...))': pour afficher le flux vidéo dans le notebook => Spécifique à Colab\n",
    "    - 'cv2.VideoCapture(0)\": pour ouvrir la webcam\n",
    "    - 'cv2.flip(frame, 1)': pour retourner l'image horizontalement (effet miroir)\n",
    "    - 'hands.process(image)': pour détecter les mains dans l'image\n",
    "    - 'mp_drawing.draw_landmarks(...)': pour dessiner les points de repère de la main sur l'image\n",
    "    - 'detect_gesture(hand_landmarks)': pour détecter les gestes à partir des points de repère\n",
    "\"\"\"\n",
    "\n",
    "def start_video_stream_with_mirror_and_landmarks():\n",
    "    # Supprimez la ligne display(HTML) car elle est spécifique à Colab\n",
    "    capture = cv2.VideoCapture(0)\n",
    "    \n",
    "    # Créer une fenêtre OpenCV\n",
    "    cv2.namedWindow('Hand Detection', cv2.WINDOW_NORMAL)\n",
    "\n",
    "    while capture.isOpened():\n",
    "        ret, frame = capture.read()\n",
    "        if not ret:\n",
    "            break\n",
    "            \n",
    "        frame = cv2.flip(frame, 1)\n",
    "        image = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = hands.process(image)\n",
    "\n",
    "        gesture = None\n",
    "        if results.multi_hand_landmarks:\n",
    "            for hand_landmarks in results.multi_hand_landmarks:\n",
    "                mp_drawing.draw_landmarks(\n",
    "                    frame, hand_landmarks, mp_hands.HAND_CONNECTIONS)\n",
    "                gesture = detect_gesture(hand_landmarks)\n",
    "\n",
    "        if gesture:\n",
    "            cv2.putText(frame, f\"Detected Gesture : {gesture}\", (10, 30),\n",
    "                        cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2, cv2.LINE_AA)\n",
    "\n",
    "        # Remplacer cv2_imshow par cv2.imshow\n",
    "        cv2.imshow('Hand Detection', frame)\n",
    "        \n",
    "        # Ajouter une condition de sortie plus claire\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "        if key == ord('q') or key == 27:  # q ou ESC pour quitter\n",
    "            break\n",
    "            \n",
    "    capture.release()\n",
    "    cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5771f254",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Lancer le programme\n",
    "start_video_stream_with_mirror_and_landmarks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f7dfcd7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nDans les prochaines étapes, je vais essayer d'introduire d'autres gestes et de les reconnaître.\\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NEXT STEPS\n",
    "\"\"\"\n",
    "Dans les prochaines étapes, je vais essayer d'introduire d'autres gestes et de les reconnaître.\n",
    "\"\"\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvComputerVision",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
